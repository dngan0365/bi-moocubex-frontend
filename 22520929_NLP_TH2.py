import math
import nltk
import re
import random
import numpy as np
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import classification_report

# Táº£i tÃ i nguyÃªn má»™t láº§n
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# DÃ¹ng láº¡i cÃ¡c Ä‘á»‘i tÆ°á»£ng
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

import pandas as pd

def read_hate_speech_file(filepath):
    rows = []
    with open(filepath, 'r', encoding='utf-8') as file:
        for line in file:
            if '\t' in line:
                content, label = line.rsplit('\t', 1)
                content = content.strip('` ').strip()
                try:
                    label = int(label.strip())
                    rows.append((content, label))
                except ValueError:
                    continue  # Skip malformed lines

    return pd.DataFrame(rows, columns=["text", "label"])


def preprocess_text(text):
    text = str(text)
    text = text.lower()
    text = text.replace("\n", " ")  # Loáº¡i bá» kÃ½ tá»± xuá»‘ng dÃ²ng
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)  # Loáº¡i bá» URL
    text = re.sub(r'[^\x00-\x7F]+', '', text)  # Loáº¡i bá» emoji vÃ  kÃ½ tá»± unicode Ä‘áº·c biá»‡t
    text = re.sub(r'\d+', '', text)  # âœ… Loáº¡i bá» sá»‘
    text = re.sub(r'[^\w\s]', '', text)  # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t (!@#...)
    text = re.sub(r'\s+', ' ', text).strip()  # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]
    return tokens

def feature(train, test):
    # Khá»Ÿi táº¡o vectorizer
    vectorizer = TfidfVectorizer(
        min_df=2,
        max_df=0.9,
        max_features=500,
        stop_words='english'
    )

    # Fit trÃªn dá»¯ liá»‡u train
    X_train = vectorizer.fit_transform(train)

    # Transform dá»¯ liá»‡u test (khÃ´ng fit láº¡i!)
    X_test = vectorizer.transform(test)

    # Láº¥y danh sÃ¡ch tá»«/ngram
    vocab = vectorizer.get_feature_names_out()
    # print(f"Sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng: {len(vocab)}")
    # print(f"VÃ­ dá»¥ tá»«/ngram: {vocab[:10]}")

    # Ghi ra file
    with open('feat.txt', 'w', encoding='utf-8') as f:
        for term in vocab:
            f.write(term + '\n')

    return X_train, X_test    


def run(test):
    df = read_hate_speech_file("data.txt")
    train = list(df["text"])
    label = list(df["label"])
    
    # Tiá»n xá»­ lÃ½ dá»¯ liá»‡u
    test = [' '.join(preprocess_text(t)) for t in test]
    train_proc = [' '.join(preprocess_text(t)) for t in train]

    # TÃ¬m Ä‘áº·c trÆ°ng
    X_train, X_test = feature(train_proc, test)
    y_train = np.array(label, dtype=int)

    # Chuyá»ƒn TF-IDF thÃ nh nhá»‹ phÃ¢n
    X_train_bin = (X_train > 0).astype(int)
    X_test_bin = (X_test > 0).astype(int)

    # Khá»Ÿi táº¡o vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh BernoulliNB
    model = BernoulliNB(alpha=0.1)
    model.fit(X_train_bin, y_train)

    # Dá»± Ä‘oÃ¡n
    y_pred = model.predict(X_test_bin)
    # y_pred_train = model.predict(X_train_bin)
    
    # Náº¿u cÃ³ nhÃ£n tháº­t, in bÃ¡o cÃ¡o
    # if y_pred_train is not None:
    #     print("\nğŸ“Š Classification Report:")
    #     print(classification_report(y_train, y_pred_train))
    

    # Ghi dá»± Ä‘oÃ¡n ra file
    with open('pred.txt', 'w', encoding='utf-8') as f:
        for l in y_pred:
            f.write(str(l) + '\n')
            
    return list(y_pred)
